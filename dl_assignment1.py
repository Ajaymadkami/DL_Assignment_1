# -*- coding: utf-8 -*-
"""DL_Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EXBB2jpnmqF_x1xtWk1TuZDSnaEzms4E
"""

!pip install wandb

import wandb
wandb.login()

"""**Question 1** : Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset."""

import numpy as np
import matplotlib.pyplot as plt
import wandb
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

# Initialize WandB
wandb.init(project="Assignment1")

# Load the dataset
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()

# Split training set into training and validation sets
split = int(0.8 * len(X_train))  # 80% for training, 20% for validation
X_validation, Y_validation = X_train[split:], Y_train[split:]
X_train, Y_train = X_train[:split], Y_train[:split]

# Flatten the images
X_train = X_train.reshape(X_train.shape[0], 28 * 28)
X_validation = X_validation.reshape(X_validation.shape[0], 28 * 28)
X_test = X_test.reshape(X_test.shape[0], 28 * 28)

# Normalize the data (scale pixel values to [0, 1])
X_train = X_train.astype('float32') / 255.0
X_validation = X_validation.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# One-hot encoding of labels
Y_train = to_categorical(Y_train, 10)
Y_validation = to_categorical(Y_validation, 10)
Y_test = to_categorical(Y_test, 10)

# Log dataset details to WandB
wandb.config.update({
    "train_size": X_train.shape[0],
    "validation_size": X_validation.shape[0],
    "test_size": X_test.shape[0],
    "image_shape": (28, 28)
})

# Class names in Fashion MNIST dataset
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Log sample images to WandB
wandb.log({"sample_images": [wandb.Image(X_train[i].reshape(28, 28), caption=class_names[Y_train[i].argmax()]) for i in range(10)]})

# Print dataset shapes
print("X_train shape:", X_train.shape)
print("Y_train shape:", Y_train.shape)
print("X_validation shape:", X_validation.shape)
print("Y_validation shape:", Y_validation.shape)
print("X_test shape:", X_test.shape)
print("Y_test shape:", Y_test.shape)

# Finish WandB run
wandb.finish()

# Plot sample images
fig, axes = plt.subplots(2, 5, figsize=(10, 5))
fig.suptitle("Sample Images from Fashion MNIST Classes", fontsize=14)

for i in range(10):
    ax = axes[i // 5, i % 5]
    idx = np.where(Y_train.argmax(axis=1) == i)[0][0]  # Find an index for each class
    ax.imshow(X_train[idx].reshape(28, 28), cmap='gray')
    ax.set_title(class_names[i])
    ax.axis('off')

plt.tight_layout()
plt.show()  # Ensure images are displayed in the output

"""**Question 2** : Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.


**Question 3 **:
Implement the backpropagation algorithm with support for the following optimisation functions

**SGD**
"""

import numpy as np
import wandb
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size):
    weights = []
    biases = []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2. / layer_sizes[i]))
        biases.append(np.zeros((1, layer_sizes[i+1])))

    return weights, biases

def apply_optimizer(weights, biases, gradients_w, gradients_b, lr):
    for i in range(len(weights)):
        weights[i] -= lr * gradients_w[i]
        biases[i] -= lr * gradients_b[i]
    return weights, biases

def forward_propagation(X, weights, biases, num_hidden_layers, activation):
    activations = [X]
    for i in range(num_hidden_layers):
        z = np.dot(activations[-1], weights[i]) + biases[i]
        a = np.maximum(0, z) if activation == 'relu' else 1 / (1 + np.exp(-z))
        activations.append(a)

    z_out = np.dot(activations[-1], weights[-1]) + biases[-1]
    exp_scores = np.exp(z_out - np.max(z_out, axis=1, keepdims=True))
    activations.append(exp_scores / np.sum(exp_scores, axis=1, keepdims=True))
    return activations

def backward_propagation(X, Y, weights, biases, activations, num_hidden_layers):
    gradients_w = [None] * len(weights)
    gradients_b = [None] * len(biases)

    delta = activations[-1] - Y
    gradients_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]
    gradients_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    for i in range(num_hidden_layers, 0, -1):
        delta = np.dot(delta, weights[i].T) * (activations[i] > 0)
        gradients_w[i-1] = np.dot(activations[i-1].T, delta) / X.shape[0]
        gradients_b[i-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    return gradients_w, gradients_b

def cross_entropy_loss(predictions, Y):
    return -np.mean(np.sum(Y * np.log(predictions + 1e-9), axis=1))

def calculate_accuracy(X_test, Y_test, weights, biases, num_hidden_layers, activation):
    activations = forward_propagation(X_test, weights, biases, num_hidden_layers, activation)
    predictions = np.argmax(activations[-1], axis=1)
    actual = np.argmax(Y_test, axis=1)
    return np.mean(predictions == actual) * 100

def sgd_optimizer(lr, X_train, Y_train, X_valid, Y_valid, epochs, activation, num_hidden_layers, num_nodes_hidden_layers, batch_size, input_size, output_size):
    wandb.init(project="Assignment1", name="SGD-training")
    weights, biases = initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size)

    for epoch in range(epochs):
        for i in range(0, X_train.shape[0], batch_size):
            x_batch = X_train[i:i+batch_size]
            y_batch = Y_train[i:i+batch_size]
            activations = forward_propagation(x_batch, weights, biases, num_hidden_layers, activation)
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, biases, activations, num_hidden_layers)
            weights, biases = apply_optimizer(weights, biases, gradients_w, gradients_b, lr)

        val_acc = calculate_accuracy(X_valid, Y_valid, weights, biases, num_hidden_layers, activation)
        loss = cross_entropy_loss(activations[-1], y_batch)
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.2f}%")
        wandb.log({"epoch": epoch + 1, "loss": loss, "validation_accuracy": val_acc})

    wandb.finish()

# Load data and preprocess
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)

# Split train into train and validation sets
split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]

# Run optimizer function
sgd_optimizer(lr=0.1, X_train=X_train, Y_train=Y_train, X_valid=X_valid, Y_valid=Y_valid,
                   epochs=10, activation='relu', num_hidden_layers=2, num_nodes_hidden_layers=128,
                   batch_size=64, input_size=28*28, output_size=10)

"""**Momentum based gradient descent**"""

import numpy as np
import wandb
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size):
    weights = []
    biases = []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2. / layer_sizes[i]))
        biases.append(np.zeros((1, layer_sizes[i+1])))

    return weights, biases

def apply_momentum(weights, biases, gradients_w, gradients_b, lr, velocity_w, velocity_b, momentum=0.9):
    for i in range(len(weights)):
        velocity_w[i] = momentum * velocity_w[i] - lr * gradients_w[i]
        velocity_b[i] = momentum * velocity_b[i] - lr * gradients_b[i]
        weights[i] += velocity_w[i]
        biases[i] += velocity_b[i]
    return weights, biases, velocity_w, velocity_b

def forward_propagation(X, weights, biases, num_hidden_layers, activation):
    activations = [X]
    for i in range(num_hidden_layers):
        z = np.dot(activations[-1], weights[i]) + biases[i]
        a = np.maximum(0, z) if activation == 'relu' else 1 / (1 + np.exp(-z))
        activations.append(a)

    z_out = np.dot(activations[-1], weights[-1]) + biases[-1]
    exp_scores = np.exp(z_out - np.max(z_out, axis=1, keepdims=True))
    activations.append(exp_scores / np.sum(exp_scores, axis=1, keepdims=True))
    return activations

def backward_propagation(X, Y, weights, biases, activations, num_hidden_layers):
    gradients_w = [None] * len(weights)
    gradients_b = [None] * len(biases)

    delta = activations[-1] - Y
    gradients_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]
    gradients_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    for i in range(num_hidden_layers, 0, -1):
        delta = np.dot(delta, weights[i].T) * (activations[i] > 0)
        gradients_w[i-1] = np.dot(activations[i-1].T, delta) / X.shape[0]
        gradients_b[i-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    return gradients_w, gradients_b

def cross_entropy_loss(predictions, Y):
    return -np.mean(np.sum(Y * np.log(predictions + 1e-9), axis=1))

def calculate_accuracy(X_test, Y_test, weights, biases, num_hidden_layers, activation):
    activations = forward_propagation(X_test, weights, biases, num_hidden_layers, activation)
    predictions = np.argmax(activations[-1], axis=1)
    actual = np.argmax(Y_test, axis=1)
    return np.mean(predictions == actual) * 100

def momentum_optimizer(lr, X_train, Y_train, X_valid, Y_valid, epochs, activation, num_hidden_layers, num_nodes_hidden_layers, batch_size, input_size, output_size, momentum=0.9):
    wandb.init(project="Assignment1", name="Momentum-training")
    weights, biases = initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size)
    velocity_w = [np.zeros_like(w) for w in weights]
    velocity_b = [np.zeros_like(b) for b in biases]

    for epoch in range(epochs):
        for i in range(0, X_train.shape[0], batch_size):
            x_batch = X_train[i:i+batch_size]
            y_batch = Y_train[i:i+batch_size]
            activations = forward_propagation(x_batch, weights, biases, num_hidden_layers, activation)
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, biases, activations, num_hidden_layers)
            weights, biases, velocity_w, velocity_b = apply_momentum(weights, biases, gradients_w, gradients_b, lr, velocity_w, velocity_b, momentum)

        val_acc = calculate_accuracy(X_valid, Y_valid, weights, biases, num_hidden_layers, activation)
        loss = cross_entropy_loss(activations[-1], y_batch)
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.2f}%")
        wandb.log({"epoch": epoch + 1, "loss": loss, "validation_accuracy": val_acc})

    wandb.finish()

# Load data and preprocess
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)

# Split train into train and validation sets
split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]

# Run momentum optimizer function
momentum_optimizer(lr=0.1, X_train=X_train, Y_train=Y_train, X_valid=X_valid, Y_valid=Y_valid,
                   epochs=10, activation='relu', num_hidden_layers=2, num_nodes_hidden_layers=128,
                   batch_size=64, input_size=28*28, output_size=10, momentum=0.9)

"""**NESTROV**"""

import numpy as np
import wandb
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size):
    weights = []
    biases = []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2. / layer_sizes[i]))
        biases.append(np.zeros((1, layer_sizes[i+1])))

    return weights, biases

def apply_nesterov(weights, biases, gradients_w, gradients_b, lr, velocity_w, velocity_b, momentum=0.9):
    for i in range(len(weights)):
        velocity_w[i] = momentum * velocity_w[i] - lr * gradients_w[i]
        velocity_b[i] = momentum * velocity_b[i] - lr * gradients_b[i]

        weights[i] += momentum * velocity_w[i] - lr * gradients_w[i]
        biases[i] += momentum * velocity_b[i] - lr * gradients_b[i]
    return weights, biases, velocity_w, velocity_b

def forward_propagation(X, weights, biases, num_hidden_layers, activation):
    activations = [X]
    for i in range(num_hidden_layers):
        z = np.dot(activations[-1], weights[i]) + biases[i]
        a = np.maximum(0, z) if activation == 'relu' else 1 / (1 + np.exp(-z))
        activations.append(a)

    z_out = np.dot(activations[-1], weights[-1]) + biases[-1]
    exp_scores = np.exp(z_out - np.max(z_out, axis=1, keepdims=True))
    activations.append(exp_scores / np.sum(exp_scores, axis=1, keepdims=True))
    return activations

def backward_propagation(X, Y, weights, biases, activations, num_hidden_layers):
    gradients_w = [None] * len(weights)
    gradients_b = [None] * len(biases)

    delta = activations[-1] - Y
    gradients_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]
    gradients_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    for i in range(num_hidden_layers, 0, -1):
        delta = np.dot(delta, weights[i].T) * (activations[i] > 0)
        gradients_w[i-1] = np.dot(activations[i-1].T, delta) / X.shape[0]
        gradients_b[i-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    return gradients_w, gradients_b

def cross_entropy_loss(predictions, Y):
    return -np.mean(np.sum(Y * np.log(predictions + 1e-9), axis=1))

def calculate_accuracy(X_test, Y_test, weights, biases, num_hidden_layers, activation):
    activations = forward_propagation(X_test, weights, biases, num_hidden_layers, activation)
    predictions = np.argmax(activations[-1], axis=1)
    actual = np.argmax(Y_test, axis=1)
    return np.mean(predictions == actual) * 100

def nesterov_optimizer(lr, X_train, Y_train, X_valid, Y_valid, epochs, activation, num_hidden_layers, num_nodes_hidden_layers, batch_size, input_size, output_size, momentum=0.9):
    wandb.init(project="Assignment1", name="Nesterov-training")
    weights, biases = initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size)
    velocity_w = [np.zeros_like(w) for w in weights]
    velocity_b = [np.zeros_like(b) for b in biases]

    for epoch in range(epochs):
        for i in range(0, X_train.shape[0], batch_size):
            x_batch = X_train[i:i+batch_size]
            y_batch = Y_train[i:i+batch_size]
            activations = forward_propagation(x_batch, weights, biases, num_hidden_layers, activation)
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, biases, activations, num_hidden_layers)
            weights, biases, velocity_w, velocity_b = apply_nesterov(weights, biases, gradients_w, gradients_b, lr, velocity_w, velocity_b, momentum)

        val_acc = calculate_accuracy(X_valid, Y_valid, weights, biases, num_hidden_layers, activation)
        loss = cross_entropy_loss(activations[-1], y_batch)
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.2f}%")
        wandb.log({"epoch": epoch + 1, "loss": loss, "validation_accuracy": val_acc})

    wandb.finish()

# Load data and preprocess
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)

# Split train into train and validation sets
split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]

# Run Nesterov optimizer function
nesterov_optimizer(lr=0.01, X_train=X_train, Y_train=Y_train, X_valid=X_valid, Y_valid=Y_valid,
                   epochs=2, activation='relu', num_hidden_layers=2, num_nodes_hidden_layers=128,
                   batch_size=64, input_size=28*28, output_size=10, momentum=0.9)

"""**RMSPROP**"""

import numpy as np
import wandb
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size):
    weights = []
    biases = []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2. / layer_sizes[i]))
        biases.append(np.zeros((1, layer_sizes[i+1])))

    return weights, biases

def apply_rmsprop(weights, biases, gradients_w, gradients_b, lr, cache_w, cache_b, beta=0.9, epsilon=1e-8):
    for i in range(len(weights)):
        cache_w[i] = beta * cache_w[i] + (1 - beta) * (gradients_w[i] ** 2)
        cache_b[i] = beta * cache_b[i] + (1 - beta) * (gradients_b[i] ** 2)

        weights[i] -= lr * gradients_w[i] / (np.sqrt(cache_w[i]) + epsilon)
        biases[i] -= lr * gradients_b[i] / (np.sqrt(cache_b[i]) + epsilon)

    return weights, biases, cache_w, cache_b

def forward_propagation(X, weights, biases, num_hidden_layers, activation):
    activations = [X]
    for i in range(num_hidden_layers):
        z = np.dot(activations[-1], weights[i]) + biases[i]
        a = np.maximum(0, z) if activation == 'relu' else 1 / (1 + np.exp(-z))
        activations.append(a)

    z_out = np.dot(activations[-1], weights[-1]) + biases[-1]
    exp_scores = np.exp(z_out - np.max(z_out, axis=1, keepdims=True))
    activations.append(exp_scores / np.sum(exp_scores, axis=1, keepdims=True))
    return activations

def backward_propagation(X, Y, weights, biases, activations, num_hidden_layers):
    gradients_w = [None] * len(weights)
    gradients_b = [None] * len(biases)

    delta = activations[-1] - Y
    gradients_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]
    gradients_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    for i in range(num_hidden_layers, 0, -1):
        delta = np.dot(delta, weights[i].T) * (activations[i] > 0)
        gradients_w[i-1] = np.dot(activations[i-1].T, delta) / X.shape[0]
        gradients_b[i-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    return gradients_w, gradients_b

def cross_entropy_loss(predictions, Y):
    return -np.mean(np.sum(Y * np.log(predictions + 1e-9), axis=1))

def calculate_accuracy(X_test, Y_test, weights, biases, num_hidden_layers, activation):
    activations = forward_propagation(X_test, weights, biases, num_hidden_layers, activation)
    predictions = np.argmax(activations[-1], axis=1)
    actual = np.argmax(Y_test, axis=1)
    return np.mean(predictions == actual) * 100

def rmsprop_optimizer(lr, X_train, Y_train, X_valid, Y_valid, epochs, activation, num_hidden_layers, num_nodes_hidden_layers, batch_size, input_size, output_size, beta=0.9, epsilon=1e-8):
    wandb.init(project="Assignment1", name="RMSprop-training")
    weights, biases = initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size)
    cache_w = [np.zeros_like(w) for w in weights]
    cache_b = [np.zeros_like(b) for b in biases]

    for epoch in range(epochs):
        for i in range(0, X_train.shape[0], batch_size):
            x_batch = X_train[i:i+batch_size]
            y_batch = Y_train[i:i+batch_size]
            activations = forward_propagation(x_batch, weights, biases, num_hidden_layers, activation)
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, biases, activations, num_hidden_layers)
            weights, biases, cache_w, cache_b = apply_rmsprop(weights, biases, gradients_w, gradients_b, lr, cache_w, cache_b, beta, epsilon)

        val_acc = calculate_accuracy(X_valid, Y_valid, weights, biases, num_hidden_layers, activation)
        loss = cross_entropy_loss(activations[-1], y_batch)
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.2f}%")
        wandb.log({"epoch": epoch + 1, "loss": loss, "validation_accuracy": val_acc})

    wandb.finish()

# Load data and preprocess
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)

# Split train into train and validation sets
split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]

# Run RMSprop optimizer function
rmsprop_optimizer(lr=0.001, X_train=X_train, Y_train=Y_train, X_valid=X_valid, Y_valid=Y_valid,
                   epochs=10, activation='relu', num_hidden_layers=2, num_nodes_hidden_layers=128,
                   batch_size=64, input_size=28*28, output_size=10, beta=0.9, epsilon=1e-8)

"""**ADAM**"""

import numpy as np
import wandb
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size):
    weights = []
    biases = []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2. / layer_sizes[i]))
        biases.append(np.zeros((1, layer_sizes[i+1])))

    return weights, biases

def apply_adam(weights, biases, gradients_w, gradients_b, lr, m_w, v_w, m_b, v_b, t, beta1=0.9, beta2=0.999, epsilon=1e-8):
    for i in range(len(weights)):
        m_w[i] = beta1 * m_w[i] + (1 - beta1) * gradients_w[i]
        v_w[i] = beta2 * v_w[i] + (1 - beta2) * (gradients_w[i] ** 2)
        m_b[i] = beta1 * m_b[i] + (1 - beta1) * gradients_b[i]
        v_b[i] = beta2 * v_b[i] + (1 - beta2) * (gradients_b[i] ** 2)

        m_w_hat = m_w[i] / (1 - beta1 ** t)
        v_w_hat = v_w[i] / (1 - beta2 ** t)
        m_b_hat = m_b[i] / (1 - beta1 ** t)
        v_b_hat = v_b[i] / (1 - beta2 ** t)

        weights[i] -= lr * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
        biases[i] -= lr * m_b_hat / (np.sqrt(v_b_hat) + epsilon)

    return weights, biases, m_w, v_w, m_b, v_b

def forward_propagation(X, weights, biases, num_hidden_layers, activation):
    activations = [X]
    for i in range(num_hidden_layers):
        z = np.dot(activations[-1], weights[i]) + biases[i]
        a = np.maximum(0, z) if activation == 'relu' else 1 / (1 + np.exp(-z))
        activations.append(a)

    z_out = np.dot(activations[-1], weights[-1]) + biases[-1]
    exp_scores = np.exp(z_out - np.max(z_out, axis=1, keepdims=True))
    activations.append(exp_scores / np.sum(exp_scores, axis=1, keepdims=True))
    return activations

def backward_propagation(X, Y, weights, biases, activations, num_hidden_layers):
    gradients_w = [None] * len(weights)
    gradients_b = [None] * len(biases)

    delta = activations[-1] - Y
    gradients_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]
    gradients_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    for i in range(num_hidden_layers, 0, -1):
        delta = np.dot(delta, weights[i].T) * (activations[i] > 0)
        gradients_w[i-1] = np.dot(activations[i-1].T, delta) / X.shape[0]
        gradients_b[i-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    return gradients_w, gradients_b

def adam_optimizer(lr, X_train, Y_train, X_valid, Y_valid, epochs, activation, num_hidden_layers, num_nodes_hidden_layers, batch_size, input_size, output_size, beta1=0.9, beta2=0.999, epsilon=1e-8):
    wandb.init(project="Assignment1", name="Adam-training")
    weights, biases = initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size)
    m_w = [np.zeros_like(w) for w in weights]
    v_w = [np.zeros_like(w) for w in weights]
    m_b = [np.zeros_like(b) for b in biases]
    v_b = [np.zeros_like(b) for b in biases]

    t = 0
    for epoch in range(epochs):
        for i in range(0, X_train.shape[0], batch_size):
            x_batch = X_train[i:i+batch_size]
            y_batch = Y_train[i:i+batch_size]
            activations = forward_propagation(x_batch, weights, biases, num_hidden_layers, activation)
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, biases, activations, num_hidden_layers)
            t += 1
            weights, biases, m_w, v_w, m_b, v_b = apply_adam(weights, biases, gradients_w, gradients_b, lr, m_w, v_w, m_b, v_b, t, beta1, beta2, epsilon)

        val_acc = calculate_accuracy(X_valid, Y_valid, weights, biases, num_hidden_layers, activation)
        loss = cross_entropy_loss(activations[-1], y_batch)
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.2f}%")
        wandb.log({"epoch": epoch + 1, "loss": loss, "validation_accuracy": val_acc})

    wandb.finish()

# Load data and preprocess
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)

# Split train into train and validation sets
split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]

# Run Adam optimizer function
adam_optimizer(lr=0.001, X_train=X_train, Y_train=Y_train, X_valid=X_valid, Y_valid=Y_valid,
               epochs=10, activation='relu', num_hidden_layers=2, num_nodes_hidden_layers=128,
               batch_size=64, input_size=28*28, output_size=10, beta1=0.9, beta2=0.999, epsilon=1e-8)

"""**NADAM**"""

import numpy as np
import wandb
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size):
    weights = []
    biases = []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2. / layer_sizes[i]))
        biases.append(np.zeros((1, layer_sizes[i+1])))

    return weights, biases

def apply_nadam(weights, biases, gradients_w, gradients_b, lr, m_w, v_w, m_b, v_b, t, beta1=0.9, beta2=0.999, epsilon=1e-8):
    for i in range(len(weights)):
        m_w[i] = beta1 * m_w[i] + (1 - beta1) * gradients_w[i]
        v_w[i] = beta2 * v_w[i] + (1 - beta2) * (gradients_w[i] ** 2)
        m_b[i] = beta1 * m_b[i] + (1 - beta1) * gradients_b[i]
        v_b[i] = beta2 * v_b[i] + (1 - beta2) * (gradients_b[i] ** 2)

        m_w_hat = (beta1 * m_w[i] + (1 - beta1) * gradients_w[i]) / (1 - beta1 ** t)
        v_w_hat = v_w[i] / (1 - beta2 ** t)
        m_b_hat = (beta1 * m_b[i] + (1 - beta1) * gradients_b[i]) / (1 - beta1 ** t)
        v_b_hat = v_b[i] / (1 - beta2 ** t)

        weights[i] -= lr * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
        biases[i] -= lr * m_b_hat / (np.sqrt(v_b_hat) + epsilon)

    return weights, biases, m_w, v_w, m_b, v_b

def nadam_optimizer(lr, X_train, Y_train, X_valid, Y_valid, epochs, activation, num_hidden_layers, num_nodes_hidden_layers, batch_size, input_size, output_size, beta1=0.9, beta2=0.999, epsilon=1e-8):
    wandb.init(project="Assignment1", name="Nadam-training")
    weights, biases = initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size)
    m_w = [np.zeros_like(w) for w in weights]
    v_w = [np.zeros_like(w) for w in weights]
    m_b = [np.zeros_like(b) for b in biases]
    v_b = [np.zeros_like(b) for b in biases]

    t = 0
    for epoch in range(epochs):
        for i in range(0, X_train.shape[0], batch_size):
            x_batch = X_train[i:i+batch_size]
            y_batch = Y_train[i:i+batch_size]
            activations = forward_propagation(x_batch, weights, biases, num_hidden_layers, activation)
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, biases, activations, num_hidden_layers)
            t += 1
            weights, biases, m_w, v_w, m_b, v_b = apply_nadam(weights, biases, gradients_w, gradients_b, lr, m_w, v_w, m_b, v_b, t, beta1, beta2, epsilon)

        val_acc = calculate_accuracy(X_valid, Y_valid, weights, biases, num_hidden_layers, activation)
        loss = cross_entropy_loss(activations[-1], y_batch)
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.2f}%")
        wandb.log({"epoch": epoch + 1, "loss": loss, "validation_accuracy": val_acc})

    wandb.finish()

# Load data and preprocess
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)

# Split train into train and validation sets
split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]

# Run Nadam optimizer function
nadam_optimizer(lr=0.001, X_train=X_train, Y_train=Y_train, X_valid=X_valid, Y_valid=Y_valid,
               epochs=10, activation='relu', num_hidden_layers=2, num_nodes_hidden_layers=128,
               batch_size=64, input_size=28*28, output_size=10, beta1=0.9, beta2=0.999, epsilon=1e-8)

"""**Question 4**
Use the sweep functionality provided by wandb to find the best values for the hyperparameters listed below. Use the standard train/test split of fashion_mnist (use (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()). Keep 10% of the training data aside as validation data for this hyperparameter search. Here are some suggestions for different values to try for hyperparameters. As you can quickly see that this leads to an exponential number of combinations. You will have to think about strategies to do this hyperparameter search efficiently. Check out the options provided by wandb.sweep and write down what strategy you chose and why.
"""

import numpy as np
import wandb
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

def apply_sgd(weights, biases, gradients_w, gradients_b, lr):
    for i in range(len(weights)):
        weights[i] -= lr * gradients_w[i]
        biases[i] -= lr * gradients_b[i]
    return weights, biases

def compute_loss_and_accuracy(X, Y, weights, biases, activation):
    predictions = forward_propagation(X, weights, biases, activation)[-1]
    loss = -np.mean(Y * np.log(predictions + 1e-8))
    acc = np.mean(np.argmax(predictions, axis=1) == np.argmax(Y, axis=1)) * 100
    return loss, acc

def train_model():
    wandb.init(project='Assignment1')
    config = wandb.config

    weights, biases, velocities_w, velocities_b, moment_w, moment_b = initialize_network(config.num_hidden_layers, config.num_hidden_nodes, 28*28, 10, config.init_method)

    for epoch in range(config.epochs):
        # Training
        for i in range(0, X_train.shape[0], config.batch_size):
            x_batch = X_train[i:i+config.batch_size]
            y_batch = Y_train[i:i+config.batch_size]
            activations = forward_propagation(x_batch, weights, biases, config.activation)
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, biases, activations, config.activation)
            weights, biases = apply_sgd(weights, biases, gradients_w, gradients_b, config.learning_rate)

        # Compute Training and Validation Metrics
        train_loss, train_acc = compute_loss_and_accuracy(X_train, Y_train, weights, biases, config.activation)
        val_loss, val_acc = compute_loss_and_accuracy(X_valid, Y_valid, weights, biases, config.activation)

        print(f"Epoch {epoch + 1}/{config.epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, "
              f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%")

        # Log Metrics to Weights & Biases
        wandb.log({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "train_accuracy": train_acc,
            "val_loss": val_loss,
            "val_accuracy": val_acc
        })

    wandb.finish()

(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)

split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]


def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size, init_method):
    weights = []
    biases = []
    velocities_w = []
    velocities_b = []
    moment_w = []
    moment_b = []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        if init_method == "xavier":
            weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(1. / layer_sizes[i]))
        else:  # Default to random
            weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01)
        biases.append(np.zeros((1, layer_sizes[i+1])))

        # Initialize velocity and moment terms for optimizers
        velocities_w.append(np.zeros_like(weights[-1]))
        velocities_b.append(np.zeros_like(biases[-1]))
        moment_w.append(np.zeros_like(weights[-1]))
        moment_b.append(np.zeros_like(biases[-1]))

    return weights, biases, velocities_w, velocities_b, moment_w, moment_b

def forward_propagation(X, weights, biases, activation):
    activations = [X]
    for i in range(len(weights) - 1):
        Z = np.dot(activations[-1], weights[i]) + biases[i]
        if activation == "relu":
            A = np.maximum(0, Z)
        elif activation == "tanh":
            A = np.tanh(Z)
        else:  # Default sigmoid
            A = 1 / (1 + np.exp(-Z))
        activations.append(A)

    Z_final = np.dot(activations[-1], weights[-1]) + biases[-1]
    exp_vals = np.exp(Z_final - np.max(Z_final, axis=1, keepdims=True))
    activations.append(exp_vals / np.sum(exp_vals, axis=1, keepdims=True))
    return activations

def backward_propagation(X, Y, weights, biases, activations, activation):
    gradients_w = [None] * len(weights)
    gradients_b = [None] * len(biases)

    dA = activations[-1] - Y

    for i in reversed(range(len(weights))):
        dZ = dA
        gradients_w[i] = np.dot(activations[i].T, dZ) / X.shape[0]
        gradients_b[i] = np.sum(dZ, axis=0, keepdims=True) / X.shape[0]
        if i > 0:
            dA = np.dot(dZ, weights[i].T)
            if activation == "relu":
                dA[activations[i] <= 0] = 0
            elif activation == "tanh":
                dA *= 1 - activations[i] ** 2
            elif activation == "sigmoid":
                dA *= activations[i] * (1 - activations[i])

    return gradients_w, gradients_b

def apply_optimizer(weights, biases, gradients_w, gradients_b, velocities_w, velocities_b, moment_w, moment_b, config, epoch):
    beta1 = 0.9
    beta2 = 0.999
    epsilon = 1e-8

    for i in range(len(weights)):
        if config.optimizer == "sgd":
            weights[i] -= config.learning_rate * gradients_w[i]
            biases[i] -= config.learning_rate * gradients_b[i]

        elif config.optimizer == "momentum":
            velocities_w[i] = beta1 * velocities_w[i] + config.learning_rate * gradients_w[i]
            velocities_b[i] = beta1 * velocities_b[i] + config.learning_rate * gradients_b[i]
            weights[i] -= velocities_w[i]
            biases[i] -= velocities_b[i]

        elif config.optimizer == "nesterov":
            temp_w = weights[i] - beta1 * velocities_w[i]
            temp_b = biases[i] - beta1 * velocities_b[i]
            velocities_w[i] = beta1 * velocities_w[i] + config.learning_rate * gradients_w[i]
            velocities_b[i] = beta1 * velocities_b[i] + config.learning_rate * gradients_b[i]
            weights[i] = temp_w - velocities_w[i]
            biases[i] = temp_b - velocities_b[i]

        elif config.optimizer == "rmsprop":
            moment_w[i] = beta2 * moment_w[i] + (1 - beta2) * (gradients_w[i] ** 2)
            moment_b[i] = beta2 * moment_b[i] + (1 - beta2) * (gradients_b[i] ** 2)
            weights[i] -= config.learning_rate * gradients_w[i] / (np.sqrt(moment_w[i]) + epsilon)
            biases[i] -= config.learning_rate * gradients_b[i] / (np.sqrt(moment_b[i]) + epsilon)

        elif config.optimizer == "adam":
            moment_w[i] = beta1 * moment_w[i] + (1 - beta1) * gradients_w[i]
            moment_b[i] = beta1 * moment_b[i] + (1 - beta1) * gradients_b[i]
            velocities_w[i] = beta2 * velocities_w[i] + (1 - beta2) * (gradients_w[i] ** 2)
            velocities_b[i] = beta2 * velocities_b[i] + (1 - beta2) * (gradients_b[i] ** 2)

            m_w_hat = moment_w[i] / (1 - beta1 ** (epoch + 1))
            m_b_hat = moment_b[i] / (1 - beta1 ** (epoch + 1))
            v_w_hat = velocities_w[i] / (1 - beta2 ** (epoch + 1))
            v_b_hat = velocities_b[i] / (1 - beta2 ** (epoch + 1))

            weights[i] -= config.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
            biases[i] -= config.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)

        elif config.optimizer == "nadam":
            m_w_hat = beta1 * moment_w[i] + (1 - beta1) * gradients_w[i]
            m_b_hat = beta1 * moment_b[i] + (1 - beta1) * gradients_b[i]
            v_w_hat = beta2 * velocities_w[i] + (1 - beta2) * (gradients_w[i] ** 2)
            v_b_hat = beta2 * velocities_b[i] + (1 - beta2) * (gradients_b[i] ** 2)

            weights[i] -= config.learning_rate * (beta1 * m_w_hat + (1 - beta1) * gradients_w[i]) / (np.sqrt(v_w_hat) + epsilon)
            biases[i] -= config.learning_rate * (beta1 * m_b_hat + (1 - beta1) * gradients_b[i]) / (np.sqrt(v_b_hat) + epsilon)

    return weights, biases

sweep_config = {
    'method': 'random',
    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},
    'parameters': {
        'epochs': {'values': [5, 10]},
        'num_hidden_layers': {'values': [3, 4, 5]},
        'num_hidden_nodes': {'values': [32, 64, 128]},
        'batch_size': {'values': [16, 32, 64]},
        'learning_rate': {'values': [0.0001, 0.001]},
        'activation': {'values': ['relu', 'tanh','sigmoid']},
        'optimizer': {'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},
        'init_method': {'values': ['xavier', 'random']}
    }
}


sweep_id = wandb.sweep(sweep=sweep_config, project='Assignment1')
wandb.agent(sweep_id, function=train_model, count=100)

"""**Question 7** : For the best model identified above, report the accuracy on the test set of fashion_mnist and plot the confusion matrix as shown below. More marks for creativity (less marks for producing the plot shown below as it is)"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from keras.datasets import fashion_mnist
from keras.utils import to_categorical
from sklearn.metrics import accuracy_score, confusion_matrix
import wandb

def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size):
    weights = []
    biases = []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(1. / layer_sizes[i]))
        biases.append(np.zeros((1, layer_sizes[i+1])))

    return weights, biases

def forward_propagation(X, weights, biases, num_hidden_layers, activation):
    activations = [X]
    for i in range(num_hidden_layers):
        z = np.dot(activations[-1], weights[i]) + biases[i]
        a = np.tanh(z)
        activations.append(a)

    z_out = np.dot(activations[-1], weights[-1]) + biases[-1]
    exp_scores = np.exp(z_out - np.max(z_out, axis=1, keepdims=True))
    activations.append(exp_scores / np.sum(exp_scores, axis=1, keepdims=True))
    return activations

def backward_propagation(X, Y, weights, biases, activations, num_hidden_layers):
    gradients_w = [None] * len(weights)
    gradients_b = [None] * len(biases)

    delta = activations[-1] - Y
    gradients_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]
    gradients_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    for i in range(num_hidden_layers, 0, -1):
        delta = np.dot(delta, weights[i].T) * (1 - activations[i] ** 2)
        gradients_w[i-1] = np.dot(activations[i-1].T, delta) / X.shape[0]
        gradients_b[i-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    return gradients_w, gradients_b

def apply_nadam(weights, biases, gradients_w, gradients_b, lr, m_w, v_w, m_b, v_b, t, beta1=0.9, beta2=0.999, epsilon=1e-8):
    for i in range(len(weights)):
        m_w[i] = beta1 * m_w[i] + (1 - beta1) * gradients_w[i]
        v_w[i] = beta2 * v_w[i] + (1 - beta2) * (gradients_w[i] ** 2)
        m_b[i] = beta1 * m_b[i] + (1 - beta1) * gradients_b[i]
        v_b[i] = beta2 * v_b[i] + (1 - beta2) * (gradients_b[i] ** 2)

        m_w_hat = m_w[i] / (1 - beta1 ** t)
        v_w_hat = v_w[i] / (1 - beta2 ** t)
        m_b_hat = m_b[i] / (1 - beta1 ** t)
        v_b_hat = v_b[i] / (1 - beta2 ** t)

        weights[i] -= lr * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
        biases[i] -= lr * m_b_hat / (np.sqrt(v_b_hat) + epsilon)

    return weights, biases, m_w, v_w, m_b, v_b

def calculate_accuracy(X, Y, weights, biases, num_hidden_layers):
    activations = forward_propagation(X, weights, biases, num_hidden_layers, activation='tanh')
    predictions = np.argmax(activations[-1], axis=1)
    labels = np.argmax(Y, axis=1)
    return accuracy_score(labels, predictions)

def plot_confusion_matrix(X, Y, weights, biases, num_hidden_layers):
    wandb.init(project="Assignment1")
    activations = forward_propagation(X, weights, biases, num_hidden_layers, activation='tanh')
    predictions = np.argmax(activations[-1], axis=1)
    labels = np.argmax(Y, axis=1)
    cm = confusion_matrix(labels, predictions)
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.savefig("confusion_matrix.png")
    wandb.log({"Confusion Matrix": wandb.Image("confusion_matrix.png")})
    plt.show()
    wandb.finish()

def nadam_optimizer(lr, X_train, Y_train, X_valid, Y_valid, X_test, Y_test, epochs, num_hidden_layers, num_nodes_hidden_layers, batch_size, input_size, output_size):
    weights, biases = initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size)
    m_w = [np.zeros_like(w) for w in weights]
    v_w = [np.zeros_like(w) for w in weights]
    m_b = [np.zeros_like(b) for b in biases]
    v_b = [np.zeros_like(b) for b in biases]

    t = 0
    for epoch in range(epochs):
        for i in range(0, X_train.shape[0], batch_size):
            x_batch = X_train[i:i+batch_size]
            y_batch = Y_train[i:i+batch_size]
            activations = forward_propagation(x_batch, weights, biases, num_hidden_layers, activation='tanh')
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, biases, activations, num_hidden_layers)
            t += 1
            weights, biases, m_w, v_w, m_b, v_b = apply_nadam(weights, biases, gradients_w, gradients_b, lr, m_w, v_w, m_b, v_b, t)

        train_acc = calculate_accuracy(X_train, Y_train, weights, biases, num_hidden_layers)
        val_acc = calculate_accuracy(X_valid, Y_valid, weights, biases, num_hidden_layers)
        print(f"Epoch {epoch + 1}/{epochs}, Train Accuracy: {train_acc:.2f}, Validation Accuracy: {val_acc:.2f}")

    test_acc = calculate_accuracy(X_test, Y_test, weights, biases, num_hidden_layers)
    print(f"Test Accuracy: {test_acc:.2f}")
    plot_confusion_matrix(X_test, Y_test, weights, biases, num_hidden_layers)

(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)

split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]

nadam_optimizer(0.001, X_train, Y_train, X_valid, Y_valid, X_test, Y_test, 10, 3, 64, 16, 28*28, 10)

import wandb

wandb.init(project="Assignment1", name="Confusion Matrix Logging")
wandb.log({"Confusion Matrix": wandb.Image("/content/confusion_matrix.png")})
wandb.finish()

"""**Question 8**
In all the models above you would have used cross entropy loss. Now compare the cross entropy loss with the squared error loss. I would again like to see some automatically generated plots or your own plots to convince me whether one is better than the other.
"""

import numpy as np
import wandb
import matplotlib.pyplot as plt
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size):
    weights = []
    biases = []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        # Xavier Initialization
        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(1. / layer_sizes[i]))
        biases.append(np.zeros((1, layer_sizes[i+1])))

    return weights, biases

def apply_momentum(weights, biases, gradients_w, gradients_b, lr, velocity_w, velocity_b, momentum=0.9):
    for i in range(len(weights)):
        velocity_w[i] = momentum * velocity_w[i] - lr * gradients_w[i]
        velocity_b[i] = momentum * velocity_b[i] - lr * gradients_b[i]
        weights[i] += velocity_w[i]
        biases[i] += velocity_b[i]
    return weights, biases, velocity_w, velocity_b

def forward_propagation(X, weights, biases, num_hidden_layers, activation, loss_type):
    activations = [X]

    # Hidden layers
    for i in range(num_hidden_layers):
        z = np.dot(activations[-1], weights[i]) + biases[i]
        a = np.maximum(0, z) if activation == 'relu' else 1 / (1 + np.exp(-z))
        activations.append(a)

    # Output layer
    z_out = np.dot(activations[-1], weights[-1]) + biases[-1]

    if loss_type == "cross_entropy":
        exp_scores = np.exp(z_out - np.max(z_out, axis=1, keepdims=True))
        activations.append(exp_scores / np.sum(exp_scores, axis=1, keepdims=True))  # Softmax
    else:
        activations.append(1 / (1 + np.exp(-z_out)))  # Sigmoid for squared error loss

    return activations

def backward_propagation(X, Y, weights, activations, num_hidden_layers, loss_type):
    gradients_w = [None] * len(weights)
    gradients_b = [None] * len(weights)

    if loss_type == "cross_entropy":
        delta = activations[-1] - Y  # Softmax derivative
    else:
        delta = (activations[-1] - Y) * (activations[-1] * (1 - activations[-1]))  # Squared Error Loss derivative

    gradients_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]
    gradients_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    for i in range(num_hidden_layers, 0, -1):
        delta = np.dot(delta, weights[i].T) * (activations[i] > 0)
        gradients_w[i-1] = np.dot(activations[i-1].T, delta) / X.shape[0]
        gradients_b[i-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]

    return gradients_w, gradients_b

def cross_entropy_loss(predictions, Y):
    return -np.mean(np.sum(Y * np.log(predictions + 1e-9), axis=1))

def squared_error_loss(predictions, Y):
    return np.mean(np.sum((predictions - Y) ** 2, axis=1))

def calculate_accuracy(X_test, Y_test, weights, biases, num_hidden_layers, activation, loss_type):
    activations = forward_propagation(X_test, weights, biases, num_hidden_layers, activation, loss_type)
    predictions = np.argmax(activations[-1], axis=1)
    actual = np.argmax(Y_test, axis=1)
    return np.mean(predictions == actual) * 100

def momentum_optimizer(lr, X_train, Y_train, X_valid, Y_valid, epochs, activation, num_hidden_layers, num_nodes_hidden_layers, batch_size, input_size, output_size, momentum=0.9, loss_type="cross_entropy"):
    wandb.init(project="Assignment1", name=f"Momentum-{loss_type}")

    weights, biases = initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size)
    velocity_w = [np.zeros_like(w) for w in weights]
    velocity_b = [np.zeros_like(b) for b in biases]

    loss_history = []
    val_acc_history = []

    for epoch in range(epochs):
        for i in range(0, X_train.shape[0], batch_size):
            x_batch = X_train[i:i+batch_size]
            y_batch = Y_train[i:i+batch_size]
            activations = forward_propagation(x_batch, weights, biases, num_hidden_layers, activation, loss_type)
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, activations, num_hidden_layers, loss_type)
            weights, biases, velocity_w, velocity_b = apply_momentum(weights, biases, gradients_w, gradients_b, lr, velocity_w, velocity_b, momentum)

        val_acc = calculate_accuracy(X_valid, Y_valid, weights, biases, num_hidden_layers, activation, loss_type)
        loss = cross_entropy_loss(activations[-1], y_batch) if loss_type == "cross_entropy" else squared_error_loss(activations[-1], y_batch)

        loss_history.append(loss)
        val_acc_history.append(val_acc)
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.2f}%")
        wandb.log({"epoch": epoch + 1, "loss": loss, "validation_accuracy": val_acc})

    wandb.finish()

    # Plot Loss and Accuracy
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(range(1, epochs+1), loss_history, label=f'{loss_type} Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'{loss_type} Loss vs. Epochs')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(1, epochs+1), val_acc_history, label='Validation Accuracy', color='green')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.title('Validation Accuracy vs. Epochs')
    plt.legend()

    plt.show()

# Load data and preprocess
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)

# Split train into train and validation sets
split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]

# Run momentum optimizer for both loss functions
momentum_optimizer(lr=0.01, X_train=X_train, Y_train=Y_train, X_valid=X_valid, Y_valid=Y_valid,
                   epochs=10, activation='relu', num_hidden_layers=2, num_nodes_hidden_layers=128,
                   batch_size=64, input_size=28*28, output_size=10, momentum=0.9, loss_type="cross_entropy")

momentum_optimizer(lr=0.01, X_train=X_train, Y_train=Y_train, X_valid=X_valid, Y_valid=Y_valid,
                   epochs=10, activation='relu', num_hidden_layers=2, num_nodes_hidden_layers=128,
                   batch_size=64, input_size=28*28, output_size=10, momentum=0.9, loss_type="squared_error")

"""### MSE SWEEP CONFIG"""

import numpy as np
import wandb
from keras.datasets import fashion_mnist
from keras.utils import to_categorical

def initialize_network(num_hidden_layers, num_nodes_hidden_layers, input_size, output_size, init_method):
    weights, biases, velocities_w, velocities_b, moment_w, moment_b = [], [], [], [], [], []
    layer_sizes = [input_size] + [num_nodes_hidden_layers] * num_hidden_layers + [output_size]

    for i in range(len(layer_sizes) - 1):
        if init_method == "xavier":
            weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(1. / layer_sizes[i]))
        else:
            weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01)
        biases.append(np.zeros((1, layer_sizes[i+1])))
        velocities_w.append(np.zeros_like(weights[-1]))
        velocities_b.append(np.zeros_like(biases[-1]))
        moment_w.append(np.zeros_like(weights[-1]))
        moment_b.append(np.zeros_like(biases[-1]))

    return weights, biases, velocities_w, velocities_b, moment_w, moment_b

def forward_propagation(X, weights, biases, activation):
    activations = [X]
    for i in range(len(weights) - 1):
        Z = np.dot(activations[-1], weights[i]) + biases[i]
        if activation == "relu":
            A = np.maximum(0, Z)
        elif activation == "tanh":
            A = np.tanh(Z)
        else:
            A = 1 / (1 + np.exp(-Z))
        activations.append(A)

    Z_final = np.dot(activations[-1], weights[-1]) + biases[-1]
    exp_vals = np.exp(Z_final - np.max(Z_final, axis=1, keepdims=True))
    activations.append(exp_vals / np.sum(exp_vals, axis=1, keepdims=True))
    return activations

def backward_propagation(X, Y, weights, biases, activations, activation):
    gradients_w, gradients_b = [None] * len(weights), [None] * len(biases)
    dA = activations[-1] - Y

    for i in reversed(range(len(weights))):
        dZ = dA
        gradients_w[i] = np.dot(activations[i].T, dZ) / X.shape[0]
        gradients_b[i] = np.sum(dZ, axis=0, keepdims=True) / X.shape[0]
        if i > 0:
            dA = np.dot(dZ, weights[i].T)
            if activation == "relu":
                dA[activations[i] <= 0] = 0
            elif activation == "tanh":
                dA *= 1 - activations[i] ** 2

    return gradients_w, gradients_b

def compute_loss(Y, predictions, loss_type):
    if loss_type == "cross_entropy":
        return -np.mean(Y * np.log(predictions + 1e-8))
    elif loss_type == "mse":
        return np.mean((Y - predictions) ** 2)

def compute_accuracy(y_true, y_pred):
    """
    Compute accuracy by comparing predicted labels with true labels.

    Args:
    - y_true: Ground truth labels (one-hot encoded or class indices).
    - y_pred: Model predictions (raw scores or probabilities).

    Returns:
    - Accuracy as a float between 0 and 1.
    """
    # Convert predictions to class indices if they are probabilities
    y_pred_labels = np.argmax(y_pred, axis=1)

    # Convert one-hot encoded true labels to class indices if needed
    if len(y_true.shape) > 1 and y_true.shape[1] > 1:
        y_true_labels = np.argmax(y_true, axis=1)
    else:
        y_true_labels = y_true  # Already class indices

    accuracy = np.mean(y_pred_labels == y_true_labels)
    return accuracy

def train_model():
    wandb.init(project='Assignment1')
    config = wandb.config

    weights, biases, velocities_w, velocities_b, moment_w, moment_b = initialize_network(
        config.num_hidden_layers, config.num_hidden_nodes, 28*28, 10, config.init_method)

    for epoch in range(config.epochs):
        for i in range(0, X_train.shape[0], config.batch_size):
            x_batch = X_train[i:i+config.batch_size]
            y_batch = Y_train[i:i+config.batch_size]

            activations = forward_propagation(x_batch, weights, biases, config.activation)
            gradients_w, gradients_b = backward_propagation(x_batch, y_batch, weights, biases, activations, config.activation)

            for j in range(len(weights)):
                weights[j] -= config.learning_rate * gradients_w[j]
                biases[j] -= config.learning_rate * gradients_b[j]

        predictions_train = forward_propagation(X_train, weights, biases, config.activation)[-1]
        predictions_val = forward_propagation(X_valid, weights, biases, config.activation)[-1]

        train_loss = compute_loss(Y_train, predictions_train, config.loss)
        val_loss = compute_loss(Y_valid, predictions_val, config.loss)

        train_acc = compute_accuracy(Y_train, predictions_train)
        val_acc = compute_accuracy(Y_valid, predictions_val)

        print(f"Epoch {epoch+1}/{config.epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

        wandb.log({"epoch": epoch+1, "train_loss": train_loss, "val_loss": val_loss, "train_acc": train_acc, "val_acc": val_acc})

    wandb.finish()

(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0
Y_train, Y_test = to_categorical(Y_train, 10), to_categorical(Y_test, 10)
split_idx = int(0.8 * X_train.shape[0])
X_valid, Y_valid = X_train[split_idx:], Y_train[split_idx:]
X_train, Y_train = X_train[:split_idx], Y_train[:split_idx]

sweep_config = {
    'method': 'random',
    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},
    'parameters': {
        'epochs': {'values': [5, 10]},
        'num_hidden_layers': {'values': [3, 4, 5]},
        'num_hidden_nodes': {'values': [32, 64, 128]},
        'batch_size': {'values': [16, 32, 64]},
        'learning_rate': {'values': [0.0001, 0.001]},
        'activation': {'values': ['relu', 'tanh', 'sigmoid']},
        'optimizer': {'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},
        'init_method': {'values': ['xavier', 'random']},
        'loss': {'values': ['mse']}  # Added loss function
    }
}


sweep_id = wandb.sweep(sweep=sweep_config, project='Assignment1')
wandb.agent(sweep_id, function=train_model, count=50)

"""**Question 10**
Based on your learnings above, give me 3 recommendations for what would work for the MNIST dataset (not Fashion-MNIST). Just to be clear, I am asking you to take your learnings based on extensive experimentation with one dataset and see if these learnings help on another dataset. If I give you a budget of running only 3 hyperparameter configurations as opposed to the large number of experiments you have run above then which 3 would you use and why. Report the accuracies that you obtain using these 3 configurations.
"""

import numpy as np
import matplotlib.pyplot as plt
import wandb
from keras.datasets import mnist
from keras.utils import to_categorical

# Initialize WandB
wandb.init(project="Assignment1")

# Load the dataset
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

# Split training set into training and validation sets
split = int(0.8 * len(X_train))  # 80% for training, 20% for validation
X_validation, Y_validation = X_train[split:], Y_train[split:]
X_train, Y_train = X_train[:split], Y_train[:split]

# Flatten the images
X_train = X_train.reshape(X_train.shape[0], 28 * 28)
X_validation = X_validation.reshape(X_validation.shape[0], 28 * 28)
X_test = X_test.reshape(X_test.shape[0], 28 * 28)

# Normalize the data (scale pixel values to [0, 1])
X_train = X_train.astype('float32') / 255.0
X_validation = X_validation.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# One-hot encoding of labels
Y_train = to_categorical(Y_train, 10)
Y_validation = to_categorical(Y_validation, 10)
Y_test = to_categorical(Y_test, 10)

# Log dataset details to WandB
wandb.config.update({
    "train_size": X_train.shape[0],
    "validation_size": X_validation.shape[0],
    "test_size": X_test.shape[0],
    "image_shape": (28, 28)
})

# Log sample images to WandB
wandb.log({"sample_images": [wandb.Image(X_train[i].reshape(28, 28), caption=str(Y_train[i].argmax())) for i in range(10)]})

# Print dataset shapes
print("X_train shape:", X_train.shape)
print("Y_train shape:", Y_train.shape)
print("X_validation shape:", X_validation.shape)
print("Y_validation shape:", Y_validation.shape)
print("X_test shape:", X_test.shape)
print("Y_test shape:", Y_test.shape)

# Finish WandB run
wandb.finish()

import numpy as np
import matplotlib.pyplot as plt
import wandb
from keras.datasets import mnist

# Initialize WandB
wandb.init(project="Assignment1_MNIST")

# Load the dataset
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

# Flatten and normalize the images
X_train = X_train.reshape(X_train.shape[0], 28 * 28) / 255.0
X_test = X_test.reshape(X_test.shape[0], 28 * 28) / 255.0

# One-hot encoding of labels
def one_hot_encode(Y, num_classes=10):
    return np.eye(num_classes)[Y]

Y_train = one_hot_encode(Y_train)
Y_test = one_hot_encode(Y_test)

# Activation functions
def relu(Z):
    return np.maximum(0, Z)

def softmax(Z):
    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))
    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)

# Xavier Initialization
def xavier_init(shape):
    return np.random.randn(*shape) * np.sqrt(2.0 / shape[0])

# Neural Network Class with Batch Normalization
class NeuralNetwork:
    def __init__(self, layer_sizes, learning_rate=0.01, optimizer='sgd'):
        self.layers = layer_sizes
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.initialize_weights()

    def initialize_weights(self):
        self.weights = {}
        for i in range(1, len(self.layers)):
            self.weights[f'W{i}'] = xavier_init((self.layers[i-1], self.layers[i]))
            self.weights[f'b{i}'] = np.zeros((1, self.layers[i]))

    def forward(self, X):
        self.cache = {"A0": X}
        for i in range(1, len(self.layers) - 1):
            Z = np.dot(self.cache[f"A{i-1}"], self.weights[f"W{i}"]) + self.weights[f"b{i}"]
            self.cache[f"A{i}"] = relu(Z)

        Z_final = np.dot(self.cache[f"A{len(self.layers)-2}"], self.weights[f"W{len(self.layers)-1}"]) + self.weights[f"b{len(self.layers)-1}"]
        self.cache[f"A{len(self.layers)-1}"] = softmax(Z_final)
        return self.cache[f"A{len(self.layers)-1}"]

    def compute_loss(self, Y_hat, Y):
        m = Y.shape[0]
        return -np.sum(Y * np.log(Y_hat + 1e-9)) / m

    def backward(self, X, Y):
        m = X.shape[0]
        grads = {}
        L = len(self.layers) - 1
        dZ = self.cache[f"A{L}"] - Y

        for i in range(L, 0, -1):
            grads[f"dW{i}"] = np.dot(self.cache[f"A{i-1}"].T, dZ) / m
            grads[f"db{i}"] = np.sum(dZ, axis=0, keepdims=True) / m
            if i > 1:
                dZ = np.dot(dZ, self.weights[f"W{i}"].T) * (self.cache[f"A{i-1}"] > 0)

        return grads

    def update_weights(self, grads):
        for i in range(1, len(self.layers)):
            self.weights[f"W{i}"] -= self.learning_rate * grads[f"dW{i}"]
            self.weights[f"b{i}"] -= self.learning_rate * grads[f"db{i}"]

    def train(self, X, Y, epochs=10, batch_size=32):
        for epoch in range(epochs):
            loss = 0
            for i in range(0, X.shape[0], batch_size):
                X_batch = X[i:i+batch_size]
                Y_batch = Y[i:i+batch_size]
                Y_hat = self.forward(X_batch)
                loss += self.compute_loss(Y_hat, Y_batch)
                grads = self.backward(X_batch, Y_batch)
                self.update_weights(grads)
            print(f"Epoch {epoch+1}, Loss: {loss/X.shape[0]}")

    def predict(self, X):
        Y_hat = self.forward(X)
        return np.argmax(Y_hat, axis=1)

# Define the corrected configurations
configs = [
    {'layers': [784, 128, 10], 'lr': 0.01, 'optimizer': 'sgd'},
    {'layers': [784, 256, 128, 10], 'lr': 0.0005, 'optimizer': 'adam'},
    {'layers': [784, 512, 256, 128, 10], 'lr': 0.0005, 'optimizer': 'rmsprop'}
]

# Train models and report accuracies
for i, config in enumerate(configs):
    print(f"\nTraining Configuration {i+1}: {config}")
    model = NeuralNetwork(layer_sizes=config['layers'], learning_rate=config['lr'], optimizer=config['optimizer'])
    model.train(X_train, Y_train, epochs=5, batch_size=64)

    Y_pred = model.predict(X_test)
    accuracy = np.mean(Y_pred == np.argmax(Y_test, axis=1))
    print(f"Test Accuracy for Config {i+1}: {accuracy:.4f}")

    wandb.log({f"accuracy_config_{i+1}": accuracy})

# Finish WandB run
wandb.finish()

